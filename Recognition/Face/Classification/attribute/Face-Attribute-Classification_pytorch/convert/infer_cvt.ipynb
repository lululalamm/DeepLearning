{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48784047-824c-49a7-9def-0e20f1874f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bf4e93-9795-4ce2-86fc-1bc1edb9fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from totalface.model_zoo.model_common import load_onnx,load_openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54893a0-1230-427f-90bd-3a0df74030e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(rgb_img,mean_list=[0.485, 0.456, 0.406],std_list=[0.229, 0.224, 0.225]):\n",
    "    MEAN = 255 * np.array(mean_list)\n",
    "    STD = 255 * np.array(std_list)\n",
    "    rgb_img = rgb_img.transpose(-1, 0, 1)\n",
    "    norm_img = (rgb_img - MEAN[:, None, None]) / STD[:, None, None]\n",
    "    \n",
    "    return norm_img\n",
    "\n",
    "def preprocessing(aimg,mean_list=[0.485, 0.456, 0.406],std_list=[0.229, 0.224, 0.225],is_onnx=False):\n",
    "    input_img = normalization(aimg,mean_list,std_list) # aimg is RGB\n",
    "    if is_onnx:\n",
    "        input_img = input_img.transpose(1,2,0)\n",
    "        return input_img\n",
    "    else:\n",
    "        input_img = torch.tensor(np.expand_dims(input_img,0).astype(np.float32))\n",
    "        return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d78dcc-9581-447b-88ad-526458cc3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14447016-8610-4194-80ff-e0e01a899fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"efficientNetB0_celebA_224.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a69b15-be93-4934-b018-ba26acd754ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "providers: ['CUDAExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "model = load_onnx.Onnx_session(path,input_mean=0.0, input_std=1.0,output_sort=True,onnx_device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5feb67-8d3f-490d-9ba8-7ec591d32e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = \"./sample.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4857d2d4-5a91-449d-88bb-01f43a4d70df",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(sample_path)\n",
    "img = preprocessing(img,is_onnx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64233e86-c5d2-44a8-abf7-4870aae761fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.03676867, 0.08534276, 0.00254387, 0.8968301 , 0.15468252,\n",
       "         0.10697055, 0.05702263, 0.00766271]], dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "197c2ce6-fadd-4b8e-8dd6-76d319f38e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openvino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4330d4f-1abf-4b0d-a954-1f77260af3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"efficientNetB0_celebA_224.vino\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8bf46e9-e6c6-431d-9046-5a8cf31b051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [path.split(\".vino\")[0]+\".xml\",path.split(\".vino\")[0]+\".bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "604dfaa8-18e7-4b27-9aec-00139f011ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_openvino.Openvino(path,not_norm=True,torch_image=True,device='CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fb12614-b17c-4dec-a0d0-358d80b03fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(sample_path)\n",
    "img = preprocessing(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e9736fd-07cd-4083-9a41-0dd53e52c7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe35b56-fd7a-455f-9480-d429556438f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
